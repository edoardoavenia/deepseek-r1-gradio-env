{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤– DeepSeek R1 Chat Interface\n",
        "\n",
        "\n",
        "## ðŸ“± [GitHub Repository](https://github.com/edoardoavenia/deepseek-r1-gradio-env)\n",
        "\n",
        "### Interactive environment to explore DeepSeek-R1 on Google Colab. This project integrates Gradio UI and LangChain pipelines, using Ollama as the model runtime to create a smooth and accessible development experience."
      ],
      "metadata": {
        "id": "0C7RBPP_Y3W4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQCQzkYtYu90"
      },
      "outputs": [],
      "source": [
        "# Update and install necessary packages\n",
        "\n",
        "!sudo apt update\n",
        "!sudo apt install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Setup\n",
        "\n",
        "### This implementation uses DeepSeek-R1 14B (deepseek-r1:14b), the largest model in the DeepSeek-R1 family that can run on Google Colab with T4 GPU (16GB). The model is a Qwen distillation optimized for reasoning.\n",
        "\n",
        "#### Important: Remember to change the runtime type to T4 GPU on Google Colab before starting."
      ],
      "metadata": {
        "id": "HCdkFC0iY9uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start Ollama service in background\n",
        "\n",
        "def run_ollama_serve():\n",
        "  subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "time.sleep(3)\n",
        "\n",
        "# Pull DeepSeek model and install required packages\n",
        "\n",
        "!ollama pull deepseek-r1:14b\n",
        "\n",
        "!pip install langchain_core langchain_ollama gradio"
      ],
      "metadata": {
        "id": "mrPGpSp6Y9Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ’¬ Conversational Interface\n",
        "\n",
        "### The following code implements a chat interface that showcases DeepSeek-R1's unique capabilities:\n",
        "- Separates the model's reasoning from its final answer using XML parsing\n",
        "- Maintains conversation history with LangChain\n",
        "- Provides a clean, user-friendly interface through Gradio\n",
        "- Uses a temperature of 0.4\n",
        "\n",
        "#### Model info and other variants available on [Ollama Library â†—](https://ollama.com/library/deepseek-r1:14b)\n"
      ],
      "metadata": {
        "id": "Y4gAWlj_Y_SH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from langchain_ollama import ChatOllama\n",
        "import xml.etree.ElementTree as ET\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# Initialize model\n",
        "llm = ChatOllama(model=\"deepseek-r1:14b\", temperature=0.4)\n",
        "\n",
        "# Conversation memory\n",
        "messages = []\n",
        "\n",
        "def respond(message, history):\n",
        "    # Add user message to history\n",
        "    messages.append(HumanMessage(content=message))\n",
        "\n",
        "    # Get model response\n",
        "    response = llm.invoke(messages)\n",
        "\n",
        "    # Parse response\n",
        "    reasoning, answer = parse_message(response.content)\n",
        "\n",
        "    # Add AI response to history\n",
        "    messages.append(AIMessage(content=answer))\n",
        "\n",
        "    return f\"# Reasoning:\\n{reasoning}\\n---\\n# Answer:\\n{answer}\"\n",
        "\n",
        "def parse_message(content):\n",
        "    try:\n",
        "        # Add root tag for valid XML\n",
        "        xml_content = f\"<root>{content}</root>\"\n",
        "        root = ET.fromstring(xml_content)\n",
        "\n",
        "        # Extract think content\n",
        "        think_content = root.find('think').text if root.find('think') is not None else \"\"\n",
        "\n",
        "        # Extract answer content\n",
        "        answer_content = \"\".join(root.itertext())\n",
        "        answer_content = answer_content.replace(think_content, \"\", 1).strip()\n",
        "\n",
        "        return think_content.strip(), answer_content\n",
        "\n",
        "    except ET.ParseError:\n",
        "        return \"\", content\n",
        "    except Exception:\n",
        "        return \"\", content\n",
        "\n",
        "# Initialize Gradio interface\n",
        "demo = gr.ChatInterface(\n",
        "    fn=respond,\n",
        "    title=\"DeepSeek R1 Chat\",\n",
        "    description=\"DeepSeek R1 â€¢ Ollama â€¢ LangChain â€¢ Gradio\",\n",
        "    theme=\"default\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "EwID1SZQZBTD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
