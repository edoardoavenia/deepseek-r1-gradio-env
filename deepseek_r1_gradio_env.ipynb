{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C7RBPP_Y3W4"
      },
      "source": [
        "# ðŸ¤– DeepSeek R1 Chat Interface\n",
        "\n",
        "\n",
        "# ðŸ“± [GitHub Repository](https://github.com/edoardoavenia/deepseek-r1-gradio-env)\n",
        "\n",
        "## Interactive environment to explore DeepSeek-R1 models on Google Colab Free GPUs. This project demonstrates the integration between Ollama, Gradio UI, and LangChain pipelines to create a smooth and accessible development experience.\n",
        "\n",
        "### Important: Remember to change the runtime type to T4 GPU on Google Colab before starting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQCQzkYtYu90"
      },
      "outputs": [],
      "source": [
        "# Update and install necessary packages\n",
        "\n",
        "!sudo apt update\n",
        "!sudo apt install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue2olExrsfmb"
      },
      "source": [
        "## ðŸ”§ Initialize Ollama Service & Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrPGpSp6Y9Rd"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import subprocess\n",
        "\n",
        "# Start Ollama service in background\n",
        "\n",
        "def run_ollama_serve():\n",
        "  subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "\n",
        "# install required packages\n",
        "\n",
        "!pip install langchain_core langchain_ollama gradio llm_xml_parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCdkFC0iY9uP"
      },
      "source": [
        "# Model Setup\n",
        "\n",
        "This implementation supports multiple DeepSeek-R1 models that can run on Google Colab's T4 GPU (16GB):\n",
        "\n",
        "- ```deepseek-r1:1.5b``` : A distilled model based on Qwen-2.5-Math-1.5B\n",
        "- ```deepseek-r1:7b``` : A distilled model based on Qwen-2.5-Math-7B\n",
        "- ```deepseek-r1:8b``` : A distilled model based on Llama-3.1-8B\n",
        "- ```deepseek-r1:14b``` : A distilled model based on Qwen-2.5-14B\n",
        "\n",
        "The default configuration uses 'deepseek-r1:1.5b', but you can scale up to the 14B model depending on your needs. Each model is optimized for reasoning tasks through distillation from their respective base models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZgUuOCErx8V"
      },
      "outputs": [],
      "source": [
        "# Pull DeepSeek model and install required packages\n",
        "\n",
        "!ollama pull deepseek-r1:1.5b  # Change with deepseek-r1:7b, deepseek-r1:8b or deepseek-r1:14b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4gAWlj_Y_SH"
      },
      "source": [
        "# ðŸ’¬ Conversational Interface\n",
        "\n",
        "## The following code implements a chat interface that showcases DeepSeek-R1's unique capabilities:\n",
        "- Separates the model's reasoning from its final answer using llm_xml_parser\n",
        "- Maintains conversation history with LangChain\n",
        "- Provides a clean, user-friendly interface through Gradio\n",
        "- Uses a temperature of 0.7\n",
        "\n",
        "## If you get any errors, try rerunning the last two code blocks\n",
        "\n",
        "#### Model info and other variants available on [Ollama Library â†—](https://ollama.com/library/deepseek-r1:1.5b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwID1SZQZBTD"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from langchain_ollama import ChatOllama\n",
        "from llm_xml_parser.core.parser import parse\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# Initialize model\n",
        "llm = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.7) # Change with deepseek-r1:7b, deepseek-r1:8b or deepseek-r1:14b\n",
        "\n",
        "# Conversation memory\n",
        "messages = []\n",
        "\n",
        "def respond(message, history):\n",
        "    # Add user message to history\n",
        "    messages.append(HumanMessage(content=message))\n",
        "\n",
        "    # Get model response\n",
        "    response = llm.invoke(messages)\n",
        "\n",
        "    # Parse response\n",
        "    config={'think': 'single'}\n",
        "\n",
        "    result = parse(response.content,config)\n",
        "\n",
        "    answer = result.untagged\n",
        "\n",
        "\n",
        "    # Add AI response to history\n",
        "    messages.append(AIMessage(content=answer))\n",
        "\n",
        "    return f\"# Reasoning:\\n{result.think}\\n---\\n# Answer:\\n{answer}\"\n",
        "\n",
        "\n",
        "# Initialize Gradio interface\n",
        "demo = gr.ChatInterface(\n",
        "    fn=respond,\n",
        "    title=\"DeepSeek R1 Chat\",\n",
        "    description=\"DeepSeek R1 â€¢ Ollama â€¢ LangChain â€¢ Gradio\",\n",
        "    theme=\"default\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
